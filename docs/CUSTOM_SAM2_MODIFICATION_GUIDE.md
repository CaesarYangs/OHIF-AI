# è‡ªå®šä¹‰ä¿®æ”¹SAM2æ¨¡å‹å¹¶é›†æˆåˆ°OHIF-AIçš„å®Œæ•´æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•ä¿®æ”¹SAM2åŸå§‹ä»£ç ï¼Œå¹¶å°†ä¿®æ”¹åçš„æ¨¡å‹é›†æˆåˆ°OHIF-AIä¸­è¿›è¡Œæ¨ç†åˆ†å‰²ã€‚

---

## ğŸ“‹ ç›®å½•

1. [SAM2ä»£ç ç»“æ„æ¦‚è§ˆ](#1-sam2ä»£ç ç»“æ„æ¦‚è§ˆ)
2. [å¸¸è§ä¿®æ”¹åœºæ™¯](#2-å¸¸è§ä¿®æ”¹åœºæ™¯)
3. [ä¿®æ”¹SAM2æ ¸å¿ƒä»£ç ](#3-ä¿®æ”¹sam2æ ¸å¿ƒä»£ç )
4. [åç«¯é›†æˆä¿®æ”¹](#4-åç«¯é›†æˆä¿®æ”¹)
5. [å‰ç«¯å‚æ•°ä¼ é€’](#5-å‰ç«¯å‚æ•°ä¼ é€’)
6. [å®Œæ•´ç¤ºä¾‹ï¼šæ·»åŠ è‡ªå®šä¹‰æç¤ºç¼–ç å™¨](#6-å®Œæ•´ç¤ºä¾‹æ·»åŠ è‡ªå®šä¹‰æç¤ºç¼–ç å™¨)
7. [è°ƒè¯•ä¸éªŒè¯](#7-è°ƒè¯•ä¸éªŒè¯)

---

## 1. SAM2ä»£ç ç»“æ„æ¦‚è§ˆ

### 1.1 ç›®å½•ç»“æ„

```
sam2/                          # SAM2æ ¹ç›®å½•
â”œâ”€â”€ sam2/                      # PythonåŒ…
â”‚   â”œâ”€â”€ __init__.py           # åŒ…å…¥å£
â”‚   â”œâ”€â”€ build_sam.py          # æ¨¡å‹æ„å»ºå…¥å£ â­
â”‚   â”œâ”€â”€ sam2_image_predictor.py  # å›¾åƒé¢„æµ‹å™¨
â”‚   â”œâ”€â”€ sam2_video_predictor.py  # è§†é¢‘é¢„æµ‹å™¨ â­â­
â”‚   â”œâ”€â”€ modeling/             # æ¨¡å‹å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ sam.py            # SAMæ¨¡å‹ä¸»ç±» â­â­
â”‚   â”‚   â”œâ”€â”€ image_encoder.py  # å›¾åƒç¼–ç å™¨ â­
â”‚   â”‚   â”œâ”€â”€ memory_encoder.py # è®°å¿†ç¼–ç å™¨
â”‚   â”‚   â”œâ”€â”€ memory_attention.py # è®°å¿†æ³¨æ„åŠ›
â”‚   â”‚   â”œâ”€â”€ prompt_encoder.py # æç¤ºç¼–ç å™¨ â­â­
â”‚   â”‚   â”œâ”€â”€ mask_decoder.py   # æ©ç è§£ç å™¨ â­â­
â”‚   â”‚   â””â”€â”€ transformer.py    # Transformeræ¨¡å—
â”‚   â””â”€â”€ utils/                # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ misc.py
â”‚       â””â”€â”€ transforms.py
â”œâ”€â”€ checkpoints/              # æ¨¡å‹æƒé‡ç›®å½•
â”œâ”€â”€ configs/                  # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ sam2/                 # SAM2é…ç½®
â”‚   â””â”€â”€ sam2.1/               # SAM2.1é…ç½® â­
â”‚       â”œâ”€â”€ sam2.1_hiera_t.yaml
â”‚       â””â”€â”€ sam2.1_hiera_s.yaml
â””â”€â”€ setup.py                  # å®‰è£…é…ç½®
```

### 1.2 å…³é”®æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | ä½œç”¨ | ä¿®æ”¹é¢‘ç‡ |
|------|------|---------|
| `sam2_video_predictor.py` | è§†é¢‘/3Dæ¨ç†å…¥å£ | â­â­â­ é«˜ |
| `sam.py` | æ¨¡å‹å‰å‘ä¼ æ’­ | â­â­â­ é«˜ |
| `prompt_encoder.py` | æç¤ºç¼–ç (ç‚¹/æ¡†) | â­â­â­ é«˜ |
| `mask_decoder.py` | æ©ç è§£ç è¾“å‡º | â­â­ ä¸­ |
| `image_encoder.py` | å›¾åƒç‰¹å¾ç¼–ç  | â­â­ ä¸­ |
| `build_sam.py` | æ¨¡å‹æ„å»º | â­ ä½ |
| `*.yaml` | æ¨¡å‹é…ç½® | â­â­ ä¸­ |

---

## 2. å¸¸è§ä¿®æ”¹åœºæ™¯

### 2.1 åœºæ™¯åˆ†ç±»

| åœºæ™¯ | ä¿®æ”¹å†…å®¹ | å½±å“èŒƒå›´ |
|-----|---------|---------|
| **A. ä¿®æ”¹æç¤ºç±»å‹** | æ·»åŠ æ–°çš„æç¤ºæ–¹å¼(å¦‚æ¶‚é¸¦ç¼–ç ) | `prompt_encoder.py` |
| **B. ä¿®æ”¹ç½‘ç»œç»“æ„** | æ·»åŠ /åˆ é™¤å±‚ï¼Œä¿®æ”¹é€šé“æ•° | `modeling/*.py` |
| **C. ä¿®æ”¹æ¨ç†é€»è¾‘** | æ”¹å˜ä¼ æ’­æ–¹å¼ï¼Œæ·»åŠ åå¤„ç† | `sam2_video_predictor.py` |
| **D. ä¿®æ”¹æŸå¤±å‡½æ•°** | è®­ç»ƒæ—¶ä½¿ç”¨çš„æŸå¤±(å¦‚éœ€è¦å¾®è°ƒ) | `modeling/sam.py` |
| **E. ä¿®æ”¹é…ç½®å‚æ•°** | å›¾åƒå°ºå¯¸ï¼Œå†…å­˜åº“å¤§å°ç­‰ | `configs/*.yaml` |

### 2.2 OHIF-AIä¸­çš„SAM2è°ƒç”¨é“¾

```
ç”¨æˆ·æç¤º â†’ frontend
    â†“
POST /monai/infer/segmentation
    â†“
basic_infer.py: __call__()
    â†“
SAMåˆ†æ”¯ (nnInter=False)
    â†“
predictor.init_state()  â† åŠ è½½3Dä½“ç§¯
    â†“
predictor.add_new_points_or_box()  â† æ·»åŠ æç¤º
    â†“
predictor.propagate_in_video()  â† 3Dä¼ æ’­
    â†“
è¿”å›åˆ†å‰²ç»“æœ
```

---

## 3. ä¿®æ”¹SAM2æ ¸å¿ƒä»£ç 

### 3.1 åœºæ™¯Aï¼šæ·»åŠ è‡ªå®šä¹‰æç¤ºç¼–ç å™¨(ä»¥æ¶‚é¸¦ä¸ºä¾‹)

**ç›®æ ‡**ï¼šè®©SAM2æ”¯æŒæ¶‚é¸¦(scribble)æç¤ºï¼Œè€Œä¸ä»…æ˜¯ç‚¹å’Œæ¡†

#### æ­¥éª¤1ï¼šä¿®æ”¹Prompt Encoder

**æ–‡ä»¶**ï¼š`sam2/modeling/prompt_encoder.py`

```python
# ========== åŸå§‹ä»£ç ç»“æ„ ==========
class PromptEncoder(nn.Module):
    def __init__(self, ...):
        self.pe_layer = PositionEmbeddingRandom(...)
        self.point_embeddings = nn.ModuleList([...])  # ç‚¹åµŒå…¥
        self.not_a_point_embed = nn.Embedding(...)    # éç‚¹åµŒå…¥
        # åªæœ‰ç‚¹å’Œæ¡†çš„ç¼–ç 
        
    def forward(self, points, boxes, masks):
        """ç¼–ç æç¤º"""
        # å¤„ç†ç‚¹å’Œæ¡†...

# ========== ä¿®æ”¹åä»£ç  ==========
class PromptEncoder(nn.Module):
    def __init__(self, embed_dim=256, ...):
        super().__init__()
        self.pe_layer = PositionEmbeddingRandom(num_pos_feats=embed_dim // 2)
        
        # åŸæœ‰åµŒå…¥
        self.point_embeddings = nn.ModuleList([
            nn.Embedding(1, embed_dim) for _ in range(4)
        ])
        self.not_a_point_embed = nn.Embedding(1, embed_dim)
        
        # ã€æ–°å¢ã€‘æ¶‚é¸¦ç¼–ç å™¨
        self.scribble_encoder = ScribbleEncoder(embed_dim)
        
        # ã€æ–°å¢ã€‘æ¶‚é¸¦åµŒå…¥
        self.scribble_embed = nn.Embedding(1, embed_dim)
        
    def forward(self, points, boxes, masks, scribbles=None):
        """
        å‚æ•°:
            points: (B, N, 2) ç‚¹åæ ‡
            boxes: (B, M, 4) æ¡†åæ ‡
            masks: (B, 1, H, W) æ©ç æç¤º
            scribbles: (B, 1, H, W) æ¶‚é¸¦æç¤º ã€æ–°å¢ã€‘
        """
        bs = self._get_batch_size(points, boxes, masks, scribbles)
        sparse_embeddings = []
        
        # ç¼–ç ç‚¹æç¤º
        if points is not None:
            point_embeddings = self._embed_points(points)
            sparse_embeddings.append(point_embeddings)
        
        # ç¼–ç æ¡†æç¤º
        if boxes is not None:
            box_embeddings = self._embed_boxes(boxes)
            sparse_embeddings.append(box_embeddings)
        
        # ã€æ–°å¢ã€‘ç¼–ç æ¶‚é¸¦æç¤º
        if scribbles is not None:
            scribble_embeddings = self.scribble_encoder(scribbles)
            sparse_embeddings.append(scribble_embeddings)
        
        # åˆå¹¶æ‰€æœ‰ç¨€ç–æç¤º
        if len(sparse_embeddings) == 0:
            sparse_embeddings = torch.zeros((bs, 0, self.embed_dim), ...)
        else:
            sparse_embeddings = torch.cat(sparse_embeddings, dim=1)
        
        # ç¼–ç æ©ç æç¤º(å¯†é›†)
        if masks is not None:
            dense_embeddings = self.mask_downscaling(masks)
        else:
            dense_embeddings = self.no_mask_embed.weight.reshape(...).expand(bs, -1, -1, -1)
        
        return sparse_embeddings, dense_embeddings


# ã€æ–°å¢ã€‘æ¶‚é¸¦ç¼–ç å™¨æ¨¡å—
class ScribbleEncoder(nn.Module):
    """
    å°†æ¶‚é¸¦æ©ç ç¼–ç ä¸ºç¨€ç–åµŒå…¥
    """
    def __init__(self, embed_dim=256, num_layers=3):
        super().__init__()
        
        # ä½¿ç”¨è½»é‡CNNç¼–ç æ¶‚é¸¦
        layers = []
        in_channels = 1
        out_channels = 64
        
        for _ in range(num_layers):
            layers.extend([
                nn.Conv2d(in_channels, out_channels, 3, padding=1),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2),
            ])
            in_channels = out_channels
            out_channels = min(out_channels * 2, 256)
        
        self.encoder = nn.Sequential(*layers)
        
        # å…¨å±€å¹³å‡æ± åŒ– + æŠ•å½±åˆ°embed_dim
        self.projection = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(out_channels, embed_dim),
            nn.LayerNorm(embed_dim),
        )
    
    def forward(self, scribbles):
        """
        å‚æ•°:
            scribbles: (B, 1, H, W) äºŒå€¼æ¶‚é¸¦æ©ç 
        è¿”å›:
            embeddings: (B, 1, embed_dim) æ¶‚é¸¦åµŒå…¥
        """
        features = self.encoder(scribbles)
        embeddings = self.projection(features)
        return embeddings.unsqueeze(1)  # (B, 1, embed_dim)
```

#### æ­¥éª¤2ï¼šä¿®æ”¹SAMæ¨¡å‹ç±»æ¥å—æ¶‚é¸¦è¾“å…¥

**æ–‡ä»¶**ï¼š`sam2/modeling/sam.py`

```python
class SAM2Base(nn.Module):
    def forward(
        self,
        image=None,
        point_coords=None,
        point_labels=None,
        boxes=None,
        mask_inputs=None,
        scribbles=None,  # ã€æ–°å¢ã€‘
        ...
    ):
        # ... åŸæœ‰ä»£ç  ...
        
        # ç¼–ç æç¤º
        sparse_embeddings, dense_embeddings = self.sam_prompt_encoder(
            points=(point_coords, point_labels),
            boxes=boxes,
            masks=mask_inputs,
            scribbles=scribbles,  # ã€æ–°å¢ã€‘ä¼ é€’æ¶‚é¸¦
        )
        
        # ... åç»­ä»£ç  ...

class SAM2VideoPredictor(SAM2Base):
    def predict(
        self,
        image,
        point_coords=None,
        point_labels=None,
        boxes=None,
        mask_input=None,
        scribbles=None,  # ã€æ–°å¢ã€‘
        ...
    ):
        """
        å•æ¬¡é¢„æµ‹
        """
        # è°ƒç”¨çˆ¶ç±»forward
        return super().forward(
            image=image,
            point_coords=point_coords,
            point_labels=point_labels,
            boxes=boxes,
            mask_inputs=mask_input,
            scribbles=scribbles,  # ã€æ–°å¢ã€‘
            ...
        )
    
    def add_new_points_or_box(
        self,
        inference_state,
        frame_idx,
        obj_id,
        points=None,
        labels=None,
        box=None,
        scribbles=None,  # ã€æ–°å¢ã€‘
    ):
        """
        æ·»åŠ æ–°çš„æç¤º(åŒ…æ‹¬æ¶‚é¸¦)
        """
        # å­˜å‚¨æ¶‚é¸¦åˆ°inference_state
        if scribbles is not None:
            inference_state["scribbles"][frame_idx] = {
                obj_id: scribbles,
            }
        
        # è°ƒç”¨åŸæœ‰é€»è¾‘
        return self._add_points_or_boxes(
            inference_state, frame_idx, obj_id,
            points, labels, box, scribbles
        )
```

#### æ­¥éª¤3ï¼šä¿®æ”¹Video Predictorå¤„ç†3Dæ¶‚é¸¦

**æ–‡ä»¶**ï¼š`sam2/sam2_video_predictor.py`

```python
class SAM2VideoPredictor:
    def add_new_points_or_box(
        self,
        inference_state,
        frame_idx,
        obj_id,
        points=None,
        labels=None,
        box=None,
        scribbles=None,  # ã€æ–°å¢ã€‘(H, W) 2Dæ¶‚é¸¦æ©ç 
        scribbles_3d=None,  # ã€æ–°å¢ã€‘(D, H, W) 3Dæ¶‚é¸¦ä½“ç§¯
    ):
        """
        æ·»åŠ æç¤ºåˆ°æŒ‡å®šå¸§
        
        æ–°å¢å‚æ•°:
            scribbles: 2Dæ¶‚é¸¦æ©ç ï¼Œç”¨äºå•å¸§
            scribbles_3d: 3Dæ¶‚é¸¦ä½“ç§¯ï¼Œç”¨äºå¤šå¸§ä¼ æ’­
        """
        # è·å–å½“å‰å¸§çš„å›¾åƒ
        img_idx = inference_state["frame_idx"][frame_idx]
        
        # å¤„ç†3Dæ¶‚é¸¦ - æå–å½“å‰å¸§çš„åˆ‡ç‰‡
        if scribbles_3d is not None:
            scribbles = scribbles_3d[frame_idx]  # æå–å¯¹åº”å¸§
        
        # é¢„å¤„ç†æ¶‚é¸¦
        if scribbles is not None:
            scribbles_tensor = self._prepare_scribble_input(scribbles)
        else:
            scribbles_tensor = None
        
        # å­˜å‚¨æç¤º
        if "prompts" not in inference_state:
            inference_state["prompts"] = {}
        
        frame_prompts = inference_state["prompts"].get(frame_idx, [])
        frame_prompts.append({
            "obj_id": obj_id,
            "points": points,
            "labels": labels,
            "box": box,
            "scribbles": scribbles_tensor,  # ã€æ–°å¢ã€‘
        })
        inference_state["prompts"][frame_idx] = frame_prompts
        
        # å¦‚æœæ˜¯æ¶‚é¸¦ï¼Œç«‹å³å¤„ç†è¯¥å¸§
        if scribbles is not None:
            # è¿è¡Œç¼–ç å™¨
            high_res_features = self._get_image_features(inference_state, frame_idx)
            
            # ç¼–ç æ¶‚é¸¦
            scribble_embedding = self.model.sam_prompt_encoder.scribble_encoder(
                scribbles_tensor
            )
            
            # ä¸å…¶ä»–æç¤ºåˆå¹¶
            # ...
        
        return self.predict(
            inference_state=inference_state,
            frame_idx=frame_idx,
            obj_id=obj_id,
            points=points,
            labels=labels,
            box=box,
            scribbles=scribbles_tensor,  # ã€æ–°å¢ã€‘
        )
    
    def _prepare_scribble_input(self, scribbles):
        """
        é¢„å¤„ç†æ¶‚é¸¦è¾“å…¥
        
        å‚æ•°:
            scribbles: numpyæ•°ç»„ (H, W) æˆ– list of points
        
        è¿”å›:
            tensor: (1, 1, H, W) å¤„ç†åçš„æ¶‚é¸¦
        """
        import torch
        import numpy as np
        
        if isinstance(scribbles, list):
            # ç‚¹åˆ—è¡¨è½¬æ¢ä¸ºæ©ç 
            # å‡è®¾scribblesæ˜¯ [[x1,y1], [x2,y2], ...]
            points = np.array(scribbles)
            H, W = inference_state["image_shape"]
            mask = np.zeros((H, W), dtype=np.float32)
            
            # ç»˜åˆ¶çº¿æ¡
            for i in range(len(points) - 1):
                cv2.line(mask, 
                        tuple(points[i]), 
                        tuple(points[i+1]), 
                        color=1, 
                        thickness=2)
            
            scribbles = mask
        
        # è½¬æ¢ä¸ºtensor
        if isinstance(scribbles, np.ndarray):
            scribbles = torch.from_numpy(scribbles).float()
        
        # æ·»åŠ batchå’Œchannelç»´åº¦
        if scribbles.dim() == 2:
            scribbles = scribbles.unsqueeze(0).unsqueeze(0)
        
        return scribbles
    
    def propagate_in_video_with_scribbles(
        self,
        inference_state,
        start_frame_idx=0,
        max_frame_num_to_track=None,
        scribbles_3d=None,  # ã€æ–°å¢ã€‘3Dæ¶‚é¸¦å¼•å¯¼ä¼ æ’­
    ):
        """
        åœ¨è§†é¢‘ä¸­ä¼ æ’­ï¼Œæ”¯æŒ3Dæ¶‚é¸¦å¼•å¯¼
        """
        # å¦‚æœæä¾›äº†3Dæ¶‚é¸¦ï¼Œåœ¨æ¯ä¸ªå¸§ä½¿ç”¨å¯¹åº”çš„åˆ‡ç‰‡
        if scribbles_3d is not None:
            for frame_idx in range(start_frame_idx, num_frames):
                if frame_idx < len(scribbles_3d):
                    # è·å–å½“å‰å¸§çš„æ¶‚é¸¦
                    frame_scribble = scribbles_3d[frame_idx]
                    
                    # æ·»åŠ æ¶‚é¸¦æç¤º
                    self.add_new_points_or_box(
                        inference_state,
                        frame_idx=frame_idx,
                        obj_id=1,
                        scribbles=frame_scribble,
                    )
        
        # ç»§ç»­æ­£å¸¸ä¼ æ’­
        yield from self.propagate_in_video(
            inference_state,
            start_frame_idx,
            max_frame_num_to_track,
        )
```

### 3.2 åœºæ™¯Bï¼šä¿®æ”¹å›¾åƒç¼–ç å™¨è¾“å‡ºç»´åº¦

**æ–‡ä»¶**ï¼š`sam2/modeling/image_encoder.py`

```python
class ImageEncoder(nn.Module):
    def __init__(
        self,
        trunk: nn.Module,
        neck: nn.Module,
        scalp: int = 1,
        custom_embed_dim: int = None,  # ã€æ–°å¢ã€‘è‡ªå®šä¹‰åµŒå…¥ç»´åº¦
    ):
        super().__init__()
        self.trunk = trunk
        self.neck = neck
        self.scalp = scalp
        
        # ã€æ–°å¢ã€‘æ·»åŠ æŠ•å½±å±‚ä¿®æ”¹è¾“å‡ºç»´åº¦
        if custom_embed_dim is not None:
            self.output_projection = nn.Sequential(
                nn.Conv2d(self.neck.embed_dim, custom_embed_dim, 1),
                nn.LayerNorm(custom_embed_dim),
            )
            self.embed_dim = custom_embed_dim
        else:
            self.embed_dim = self.neck.embed_dim
    
    def forward(self, sample: torch.Tensor):
        """
        å‚æ•°:
            sample: (B, 3, H, W) è¾“å…¥å›¾åƒ
        è¿”å›:
            features: (B, embed_dim, H', W') ç‰¹å¾å›¾
        """
        # ä¸»å¹²ç½‘ç»œ
        x = self.trunk(sample)
        
        # é¢ˆéƒ¨å¤„ç†
        x = self.neck(x)
        
        # ã€æ–°å¢ã€‘è¾“å‡ºæŠ•å½±
        if hasattr(self, 'output_projection'):
            x = self.output_projection(x)
        
        return x
```

### 3.3 åœºæ™¯Cï¼šä¿®æ”¹ä¼ æ’­é€»è¾‘(æ·»åŠ åå¤„ç†)

**æ–‡ä»¶**ï¼š`sam2/sam2_video_predictor.py`

```python
class SAM2VideoPredictor:
    def propagate_in_video(
        self,
        inference_state,
        start_frame_idx=0,
        max_frame_num_to_track=None,
        reverse=False,
        post_process_mask=True,  # ã€æ–°å¢ã€‘æ©ç åå¤„ç†
        apply_3d_morphology=True,  # ã€æ–°å¢ã€‘3Då½¢æ€å­¦å¤„ç†
    ):
        """
        åœ¨è§†é¢‘ä¸­ä¼ æ’­åˆ†å‰²
        
        æ–°å¢å‚æ•°:
            post_process_mask: æ˜¯å¦åº”ç”¨åå¤„ç†
            apply_3d_morphology: æ˜¯å¦åº”ç”¨3Då½¢æ€å­¦æ“ä½œ
        """
        # æ”¶é›†æ‰€æœ‰å¸§çš„ç»“æœ
        results = []
        for frame_idx, obj_ids, mask_logits in self._propagate_frames(...):
            
            # ã€æ–°å¢ã€‘åå¤„ç†
            if post_process_mask:
                mask_logits = self._post_process_mask(mask_logits)
            
            results.append((frame_idx, obj_ids, mask_logits))
            yield frame_idx, obj_ids, mask_logits
        
        # ã€æ–°å¢ã€‘3Då½¢æ€å­¦å¤„ç†(æ•´ä¸ªä½“ç§¯)
        if apply_3d_morphology:
            self._apply_3d_morphology(results)
    
    def _post_process_mask(self, mask_logits):
        """
        å•å¸§æ©ç åå¤„ç†
        """
        import torch.nn.functional as F
        
        # 1. é˜ˆå€¼åŒ–
        mask = (mask_logits > 0.0).float()
        
        # 2. å¡«å……å°å­”
        from scipy import ndimage
        mask_np = mask.cpu().numpy()
        mask_np = ndimage.binary_fill_holes(mask_np)
        
        # 3. ç§»é™¤å°è¿é€šåŸŸ
        mask_np = self._remove_small_regions(mask_np, min_area=100)
        
        return torch.from_numpy(mask_np).to(mask_logits.device)
    
    def _remove_small_regions(self, mask, min_area=100):
        """ç§»é™¤å°çš„è¿é€šåŸŸ"""
        from scipy import ndimage
        labeled, num_features = ndimage.label(mask)
        
        for i in range(1, num_features + 1):
            region = (labeled == i)
            if region.sum() < min_area:
                mask[region] = 0
        
        return mask
    
    def _apply_3d_morphology(self, results):
        """
        å¯¹æ•´ä¸ª3Dä½“ç§¯åº”ç”¨å½¢æ€å­¦æ“ä½œ
        """
        import scipy.ndimage as ndi
        
        # æ”¶é›†æ‰€æœ‰æ©ç 
        masks = [r[2].cpu().numpy() for r in results]
        volume = np.stack(masks, axis=0)  # (D, H, W)
        
        # 3Dé—­è¿ç®—(å¡«å……é—´éš™)
        volume = ndi.binary_closing(volume, structure=np.ones((3, 3, 3)))
        
        # 3Då¼€è¿ç®—(å»é™¤å™ªç‚¹)
        volume = ndi.binary_opening(volume, structure=np.ones((3, 3, 3)))
        
        # æ›´æ–°ç»“æœ
        for i, (frame_idx, obj_ids, _) in enumerate(results):
            results[i] = (frame_idx, obj_ids, torch.from_numpy(volume[i]))
```

---

## 4. åç«¯é›†æˆä¿®æ”¹

### 4.1 ä¿®æ”¹basic_infer.pyæ”¯æŒè‡ªå®šä¹‰SAM2

**æ–‡ä»¶**ï¼š`monai-label/monailabel/tasks/infer/basic_infer.py`

```python
# ========== ä¿®æ”¹åçš„SAM2åˆå§‹åŒ– ==========

# åˆ¤æ–­ä½¿ç”¨å“ªä¸ªç‰ˆæœ¬çš„SAM2
USE_CUSTOM_SAM2 = os.environ.get("USE_CUSTOM_SAM2", "false").lower() == "true"
CUSTOM_SAM2_PATH = os.environ.get("CUSTOM_SAM2_PATH", "/code/custom_sam2")

if USE_CUSTOM_SAM2:
    # åŠ è½½ä¿®æ”¹åçš„SAM2
    import sys
    sys.path.insert(0, CUSTOM_SAM2_PATH)
    
    from custom_sam2.build_sam import build_sam2_video_predictor
    from custom_sam2.sam2_video_predictor import SAM2VideoPredictor
    
    print("Using CUSTOM SAM2 from:", CUSTOM_SAM2_PATH)
else:
    # ä½¿ç”¨åŸç‰ˆSAM2
    from sam2.build_sam import build_sam2_video_predictor

# åˆå§‹åŒ–æ—¶æ·»åŠ è‡ªå®šä¹‰å‚æ•°
sam2_config = {
    "model_cfg": "configs/sam2.1/sam2.1_hiera_t.yaml",
    "checkpoint": "/code/checkpoints/sam2.1_hiera_tiny.pt",
    # ã€æ–°å¢ã€‘è‡ªå®šä¹‰å‚æ•°
    "custom_embed_dim": 256,  # å¦‚æœä¿®æ”¹äº†ç¼–ç å™¨è¾“å‡º
    "enable_scribble": True,  # å¯ç”¨æ¶‚é¸¦æ”¯æŒ
    "post_process_mask": True,  # å¯ç”¨åå¤„ç†
}

# ä½¿ç”¨è‡ªå®šä¹‰æ„å»ºå‡½æ•°
if USE_CUSTOM_SAM2:
    predictor_sam2 = build_sam2_video_predictor(
        sam2_config["model_cfg"],
        sam2_config["checkpoint"],
        custom_embed_dim=sam2_config.get("custom_embed_dim"),
        enable_scribble=sam2_config.get("enable_scribble", False),
    )
else:
    predictor_sam2 = build_sam2_video_predictor(
        sam2_config["model_cfg"],
        sam2_config["checkpoint"],
    )


# ========== ä¿®æ”¹æ¨ç†æµç¨‹ä¼ é€’æ¶‚é¸¦ ==========

def _sam_infer(self, img, data, contrast_center, contrast_window):
    """SAM2æ¨ç†ï¼Œæ”¯æŒæ¶‚é¸¦"""
    
    # é€‰æ‹©é¢„æµ‹å™¨(åŸç‰ˆæˆ–è‡ªå®šä¹‰)
    if data.get('use_custom_sam2', False) and USE_CUSTOM_SAM2:
        predictor = predictor_sam2_custom
    else:
        predictor = predictor_sam2
    
    # åˆå§‹åŒ–çŠ¶æ€
    with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
        inference_state = predictor.init_state(
            video_path=img,
            clip_low=contrast_center - contrast_window/2 if contrast_center else None,
            clip_high=contrast_center + contrast_window/2 if contrast_center else None,
        )
    
    # å¤„ç†ç‚¹æç¤º
    if len(result_json.get('pos_points', [])) > 0:
        for frame_idx in ann_frame_list:
            # ... åŸæœ‰ä»£ç  ...
            
            # ã€æ–°å¢ã€‘å¦‚æœè‡ªå®šä¹‰SAM2æ”¯æŒæ¶‚é¸¦ä¸”å‰ç«¯ä¼ é€’äº†æ¶‚é¸¦
            if USE_CUSTOM_SAM2 and 'scribbles' in data:
                scribble_mask = self._prepare_scribble_mask(
                    data['scribbles'], 
                    img_np.shape
                )
                
                predictor.add_new_points_or_box(
                    inference_state=inference_state,
                    frame_idx=frame_idx,
                    obj_id=ann_obj_id,
                    points=points,
                    labels=labels,
                    scribbles=scribble_mask,  # ã€æ–°å¢ã€‘ä¼ é€’æ¶‚é¸¦
                )
            else:
                # åŸç‰ˆSAM2ï¼Œåªä¼ é€’ç‚¹å’Œæ¡†
                predictor.add_new_points_or_box(
                    inference_state=inference_state,
                    frame_idx=frame_idx,
                    obj_id=ann_obj_id,
                    points=points,
                    labels=labels,
                    box=boxes if len(boxes) > 0 else None,
                )
    
    # ä¼ æ’­æ—¶å¯ç”¨åå¤„ç†
    video_segments = {}
    with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
        
        # ã€æ–°å¢ã€‘æ£€æŸ¥æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰ä¼ æ’­
        if USE_CUSTOM_SAM2 and hasattr(predictor, 'propagate_in_video_with_scribbles'):
            # ä½¿ç”¨è‡ªå®šä¹‰ä¼ æ’­(æ”¯æŒ3Dæ¶‚é¸¦)
            generator = predictor.propagate_in_video_with_scribbles(
                inference_state,
                scribbles_3d=data.get('scribbles_3d'),  # ã€æ–°å¢ã€‘
                post_process_mask=True,  # ã€æ–°å¢ã€‘
                apply_3d_morphology=data.get('apply_3d_morphology', False),  # ã€æ–°å¢ã€‘
            )
        else:
            # åŸç‰ˆä¼ æ’­
            generator = predictor.propagate_in_video(inference_state)
        
        for out_frame_idx, out_obj_ids, out_mask_logits in generator:
            video_segments[out_frame_idx] = {
                obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()
                for i, obj_id in enumerate(out_obj_ids)
            }
    
    return pred, {"sam_elapsed": elapsed_time, ...}


def _prepare_scribble_mask(self, scribbles_data, image_shape):
    """
    å°†å‰ç«¯ä¼ æ¥çš„æ¶‚é¸¦æ•°æ®è½¬æ¢ä¸ºSAM2å¯ç”¨çš„æ©ç 
    
    å‚æ•°:
        scribbles_data: ä»å‰ç«¯ä¼ æ¥çš„æ¶‚é¸¦åæ ‡åˆ—è¡¨
        image_shape: (H, W, D) æˆ– (H, W)
    
    è¿”å›:
        scribble_mask: (D, H, W) 3Dæ¶‚é¸¦æ©ç æˆ– (H, W) 2Dæ©ç 
    """
    if len(image_shape) == 3:
        H, W, D = image_shape
    else:
        H, W = image_shape
        D = 1
    
    # åˆ›å»º3Dæ©ç 
    scribble_mask = np.zeros((D, H, W), dtype=np.float32)
    
    # å¡«å……æ¶‚é¸¦ç‚¹
    for point in scribbles_data:
        x, y, z = int(point[0]), int(point[1]), int(point[2])
        if 0 <= z < D and 0 <= y < H and 0 <= x < W:
            # ä»¥ç‚¹ä¸ºä¸­å¿ƒç»˜åˆ¶å°åœ†
            cv2.circle(scribble_mask[z], (x, y), radius=2, color=1, thickness=-1)
    
    return torch.from_numpy(scribble_mask).unsqueeze(1)  # (D, 1, H, W)
```

### 4.2 æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹é…ç½®

**æ–‡ä»¶**ï¼š`monai-label/monailabel/config.py`

```python
class Settings(BaseSettings):
    # ... åŸæœ‰é…ç½® ...
    
    # ã€æ–°å¢ã€‘è‡ªå®šä¹‰SAM2é…ç½®
    USE_CUSTOM_SAM2: bool = False
    CUSTOM_SAM2_PATH: str = "/code/custom_sam2"
    SAM2_CUSTOM_EMBED_DIM: int = 256
    SAM2_ENABLE_SCRIBBLE: bool = False
    SAM2_POST_PROCESS: bool = True
```

---

## 5. å‰ç«¯å‚æ•°ä¼ é€’

### 5.1 ä¿®æ”¹è¯·æ±‚å‚æ•°

**æ–‡ä»¶**ï¼š`Viewers/modes/longitudinal/src/toolbarButtons.ts`

```typescript
// æ·»åŠ è‡ªå®šä¹‰SAM2é€‰é¡¹
const sam2ModelSelector = {
  id: 'sam2ModelSelector',
  label: 'SAM2 Version',
  type: 'radioGroup',
  options: [
    { label: 'Standard SAM2', value: 'standard' },
    { label: 'Custom SAM2 (Scribble)', value: 'custom_scribble' },
    { label: 'Custom SAM2 (3D Morph)', value: 'custom_3d' },
  ],
  commandName: 'setSAM2Version',
};

// æ·»åŠ åå¤„ç†é€‰é¡¹
const postProcessToggle = {
  id: 'sam2PostProcess',
  label: 'Enable Post-processing',
  type: 'toggle',
  defaultValue: true,
};
```

### 5.2 ä¿®æ”¹APIè°ƒç”¨

**æ–‡ä»¶**ï¼šå‰ç«¯AIæœåŠ¡

```typescript
class AIService {
  async runInference(requestData: InferenceRequest) {
    const params = {
      ...requestData,
      // ä¼ é€’è‡ªå®šä¹‰SAM2å‚æ•°
      use_custom_sam2: this.selectedModel === 'custom_scribble',
      apply_3d_morphology: this.selectedModel === 'custom_3d',
      enable_post_process: this.postProcessEnabled,
      
      // å¦‚æœæœ‰æ¶‚é¸¦ï¼Œä¼ é€’æ¶‚é¸¦æ•°æ®
      scribbles: this.collectedScribbles,
      scribbles_3d: this.convertScribblesTo3D(this.collectedScribbles),
    };
    
    const response = await fetch('/monai/infer/segmentation', {
      method: 'POST',
      body: JSON.stringify(params),
    });
    
    return response;
  }
  
  private convertScribblesTo3D(scribbles: Point2D[]): Scribble3D {
    // å°†2Dæ¶‚é¸¦åˆ—è¡¨è½¬æ¢ä¸º3Dä½“ç§¯
    const volume = new Float32Array(depth * height * width);
    
    scribbles.forEach(point => {
      const idx = point.z * height * width + point.y * width + point.x;
      volume[idx] = 1.0;
    });
    
    return volume;
  }
}
```

---

## 6. å®Œæ•´ç¤ºä¾‹ï¼šæ·»åŠ è‡ªå®šä¹‰æç¤ºç¼–ç å™¨

### æ­¥éª¤æ€»ç»“

```bash
# 1. å¤åˆ¶SAM2ä»£ç åˆ°è‡ªå®šä¹‰ç›®å½•
cp -r sam2 custom_sam2

# 2. ä¿®æ”¹ custom_sam2/modeling/prompt_encoder.py
#    - æ·»åŠ  ScribbleEncoder ç±»
#    - ä¿®æ”¹ forward æ–¹æ³•æ¥å— scribbles å‚æ•°

# 3. ä¿®æ”¹ custom_sam2/modeling/sam.py
#    - ä¿®æ”¹ forward æ–¹æ³•ä¼ é€’ scribbles
#    - ä¿®æ”¹ predict æ–¹æ³•

# 4. ä¿®æ”¹ custom_sam2/sam2_video_predictor.py
#    - ä¿®æ”¹ add_new_points_or_box æ¥å— scribbles
#    - æ·»åŠ  propagate_in_video_with_scribbles

# 5. é‡æ–°å®‰è£…è‡ªå®šä¹‰SAM2
cd custom_sam2
pip install -e .

# 6. ä¿®æ”¹åç«¯ basic_infer.py
#    - æ·»åŠ  USE_CUSTOM_SAM2 åˆ¤æ–­
#    - ä¿®æ”¹ _sam_infer ä¼ é€’æ¶‚é¸¦å‚æ•°

# 7. ä¿®æ”¹å‰ç«¯ä¼ é€’æ¶‚é¸¦æ•°æ®

# 8. æµ‹è¯•
```

---

## 7. è°ƒè¯•ä¸éªŒè¯

### 7.1 å•å…ƒæµ‹è¯•

```python
def test_custom_prompt_encoder():
    from custom_sam2.modeling.prompt_encoder import PromptEncoder, ScribbleEncoder
    
    # æµ‹è¯•æ¶‚é¸¦ç¼–ç å™¨
    encoder = ScribbleEncoder(embed_dim=256)
    scribble = torch.randn(1, 1, 256, 256)
    
    embedding = encoder(scribble)
    assert embedding.shape == (1, 1, 256)


def test_custom_sam2_forward():
    from custom_sam2.build_sam import build_sam2_video_predictor
    
    predictor = build_sam2_video_predictor(
        "configs/sam2.1/sam2.1_hiera_t.yaml",
        "checkpoints/sam2.1_hiera_tiny.pt",
        enable_scribble=True,
    )
    
    # æµ‹è¯•æ¶‚é¸¦è¾“å…¥
    scribbles = torch.zeros(1, 1, 256, 256)
    scribbles[0, 0, 100:150, 100:150] = 1.0
    
    result = predictor.predict(
        image=torch.randn(1, 3, 1024, 1024),
        scribbles=scribbles,
    )
    
    assert result is not None
```

### 7.2 é›†æˆéªŒè¯

```bash
# 1. æ£€æŸ¥è‡ªå®šä¹‰SAM2æ˜¯å¦æ­£ç¡®åŠ è½½
python -c "from custom_sam2.sam2_video_predictor import SAM2VideoPredictor; print('OK')"

# 2. æ£€æŸ¥åç«¯æ˜¯å¦èƒ½å¯¼å…¥
python -c "from monailabel.tasks.infer.basic_infer import predictor_sam2; print(type(predictor_sam2))"

# 3. å‘é€æµ‹è¯•è¯·æ±‚
curl -X POST http://localhost:8002/monai/infer/segmentation \
  -F 'params={"use_custom_sam2": true, "scribbles": [[100,100,50]], "pos_points": [[100,100,50]]}'
```

---

## 8. æ€»ç»“

### ä¿®æ”¹æ¸…å•

| ç»„ä»¶ | æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ |
|------|------|---------|
| **SAM2æ ¸å¿ƒ** | `modeling/prompt_encoder.py` | æ·»åŠ æ¶‚é¸¦ç¼–ç å™¨ |
| **SAM2æ ¸å¿ƒ** | `modeling/sam.py` | ä¿®æ”¹forwardæ¥å—æ¶‚é¸¦ |
| **SAM2æ ¸å¿ƒ** | `sam2_video_predictor.py` | æ·»åŠ æ¶‚é¸¦å¤„ç†æ–¹æ³• |
| **åç«¯** | `basic_infer.py` | æ”¯æŒè‡ªå®šä¹‰SAM2åŠ è½½å’Œè°ƒç”¨ |
| **åç«¯** | `config.py` | æ·»åŠ è‡ªå®šä¹‰SAM2é…ç½® |
| **å‰ç«¯** | `toolbarButtons.ts` | æ·»åŠ ç‰ˆæœ¬é€‰æ‹©å™¨ |
| **å‰ç«¯** | AIæœåŠ¡ | ä¼ é€’æ¶‚é¸¦å‚æ•° |

### å…³é”®æ³¨æ„äº‹é¡¹

1. **ç‰ˆæœ¬å…¼å®¹æ€§**ï¼šç¡®ä¿è‡ªå®šä¹‰SAM2ä¸åŸç‰ˆAPIå…¼å®¹
2. **æƒé‡åŠ è½½**ï¼šä¿®æ”¹ç»“æ„åå¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒ
3. **æ€§èƒ½å½±å“**ï¼šå¤æ‚ç¼–ç å™¨å¯èƒ½å½±å“æ¨ç†é€Ÿåº¦
4. **CUDAå†…å­˜**ï¼š3Då¤„ç†æ³¨æ„æ˜¾å­˜ç®¡ç†

---

*æœ¬æ–‡æ¡£æä¾›äº†å®Œæ•´çš„SAM2è‡ªå®šä¹‰ä¿®æ”¹æ–¹æ¡ˆï¼Œæ›´å¤šç»†èŠ‚è¯·å‚è€ƒå…·ä½“ä»£ç å®ç°*
